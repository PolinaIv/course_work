\chapter{Теоретическая часть}\label{chp:theory}
Теоретические основы технологии HTM базируются на биологической теории строения коры головного мозга, описанной Д. Хокинсом в \cite{on_intelligence}. Согласно нейробиологической теории, которой придерживается Хокинса, неокортекс осуществляет основные функции разума живого существа. Особенности памяти неокортекса заключаются в сохранении временных и пространственных последовательностей, автоассоциативном запоминании объектов и в инвариантных представлениях объектов. 

\section{Структура HTM}\label{sec:theory_HTM_structure}
Структура неокортекса практически однородна: он представляет собой шесть связанных слоев нейронов (иногда говорят о пяти, объединяя вместе второй и третий слои), общей толщиной около 2 мм.  В первом, верхнем, слое, в отличие от остальных, наблюдаются в основном аксоны и незначительное число нейронов. Остальные слои различаются между собой по многим характеристикам: толщина слоя, форма составляющих его нейронов, связанными со слоем органами или другими областями коры.

Не менее важным принципом организации неокортекса является его колончатая структура: нейроны, реагирующие на сигнал одинаковым образом, выстраиваются  в кортикальные колонки, идущие через все слои.

Разумеется, что полностью запрограммировать структуру неокортекса невозможно. Однако основные принципы биологической организации в HTM сохранены.
Для описания элементов сетей HTM используются базовые понятия из нейрофизиологии. 
Как видно из названия, одним из основных принципов является иерархическое строение сетей HTM. Каждый уровень иерархии содержит <<регионы>>,  то есть единиц памяти, осуществляющих функции запоминания и предсказания. Регион HTM подобен региону неокортекса: слои сильно взаимосвязанных клеток HTM организуются в колонки.

Клетки в регионах HTM являются аналогами нейронов: они имеют дендритные сегменты и синапсы,  с помощью которых осуществляется передача сигнала в сети, и, соответственно, обучение.  Клетки в сети, как и в неокортексе, организованы в горизонтальные слои и вертикальные колонки. Таким образом, регион HTM-сети можно представлять как трехмерный массив. Однако, в отличии от неокортекса, число слоев и клеток является параметрами сети и может быть задано пользователем. Стандартно используют четыре слоя.

В нейрофизиологии отдельный нейрон с дивергентной структурой может посылать сигналы тысяче и даже большему числу других нейронов. Но чаще один такой нейрон соединяется всего лишь с несколькими определенными нейронами. \cite{bloom} 
В сети HTM рассматривается подобная организация. Отдельные клетки передают сигнал с помощью дендритных сегментов и синапсов. 

Дендритные сегменты клетки разделяются на один близкий дендрит (проксимальный) и десятки удаленных (дистальных). Первый сегмент является общим для всей колонки клеток и принимает информацию извне региона сети, а через удаленные дендриты поступает информация от других ближайших клеток и колонок.

Синапсы в HTM --- связи между клетками. Формирование синапсов в HTM основывается на следующих концепциях из нейробиологии: концепция потенциальных синапсов и концепция перманентности синапса. Первая говорит, что физически возможны лишь некоторые соединения между клетками, поэтому у каждого дедритного сегмента есть список потенциальных синапсов, каждый из которых характеризуется перманентностью. 

\textbf{Перманентность} --- число в интервале от 0 до 1, характеризующее прочность связи потенциального синапса. Синапсы, в отличие от своих аналогов в биологии или в математических моделях нейронных сетей, имеют бинарный вес, который определяется как следующий индикатор: $\mathbb{1} \left( permanence \ge threshold \right)$. Обучение HTM сети основано на постоянном изменении значений перманентности. 

Сами клетки также характеризуются одним из состояний: 
\begin{enumerate}
\item активность от прямого воздействия (клетка <<активна>>),
\item активность от бокового воздействия, то есть от соседних клеток в регионе (клетка <<в состоянии предсказания>>),
\item клетка не активна. 
\end{enumerate}

\section{Обучение и предсказание в HTM}
Как правило, на вход сетей HTM подаются данные, непрерывно изменяющиеся во времени. Как уже было сказано, в самой сети они хранятся подобно разреженным трехмерным матрицам. Такие представления информации называются пространственно-разреженными, или Sparse Distributed Representations (SDR). \cite{htm_documentation} HTM допускает стандартные типы входных данных: бинарные или скалярные, категориальные данные, дата и время. Если представить входящую информацию как набор бит, то для каждой из колонок региона будет зафиксирована уникальная часть набора, активность которой колонка будет отслеживать. Такие части перекрываются между собой, но не совпадают полностью друг с другом. При активации бита (когда его значение становится равным 1), колонка или часть клеток в колонке становится активными, тем самым кодируя входной сигнал бинарным представлением.

Как и в слоях неокортекса, где присутствуют подавляющие нейроны, в сети HTM колонки с большим уровнем активности подавляют колонки с меньшим уровнем, не позволяя при поступлении сигнала активироваться одновременно большому числу связанных нейронов. Поэтому поступающая информация кодируется в сети HTM лишь небольшим числом одновременно активных клеток из всех имеющихся в регионе. 

Пространственно-разреженные представления имеют ряд преимуществ. Если входной сигнал немного изменился, то часть колонок будут воспринимать немного больше или меньше информации, но бОльшая часть активных клеток останется той же. Поэтому похожим входным данным буду соответствовать похожие пространственно-разреженные представления. Рассмотрим поясняющий  пример на двумерных бинарных матриц. Допустим (рис. ...), для кодирования информации доступно $n = 36$ битов, из которым одновременно активными будут до $w = 9$ бит. Если к исходному образцу представлению добавили некоторый шум, то множества одновременно активных битов будут не совпадать. Допустим из 21 активного бита в обоих представлениях по своим позициям совпали $b = 5$ битов. Тогда вероятность того, что  в качестве второго представления мы наблюдаем не зашумленное первое, а совершенно иное будет мала: $\frac{|\Omega_x (n,w,b)| }{\binom nk} \approx 0.023$, где  $|\Omega_x (n,w,b)|$ –- мощность множества матриц размера $(n x n)$ с $w$ активными битами, пересекающихся с нашим фиксированным пространственным представлением в $b$ активных битов, а ${\binom nk}$ --- число всевозможных пространственно-разреженных представлений с числом активных битов, равным $w$. 
Типичная ситуация, когда в качестве значений используют n= 2048, w = 21. Тогда при совпадении в 15 битах вероятность ошибиться будет только  $8.5 \cdot 10^{-29}$.  

Вернемся к механизму обучения регионов HTM. Регион обучается путем установления во входных данных некоторых шаблонов, а также их последовательностей. Такие шаблоны делятся на пространственные и временные. Пространственные шаблоны --- комбинация часто активных одновременно битов. Последовательности в пространственных шаблонов образуют временные. 

Регион HTM ограничен в памяти, а следовательно и в числе запоминающихся им шаблонов. Это число также варьируется в зависимости от сложности входных данных.

Обучение региона HTM способно проходить непрерывно. Поэтому, в отличие от многих методов машинного обучения, не  требуется разделение имеющихся данных на тренировочную и тестовую выборки. Однако, при необходимости, такой подход может быть реализован: самообучение региона можно отключить после построения модели на тренировочной выборке или после окончания обучения низших (относительно рассматриваемого региона) слоев иерархии.
 
Для успешного обучения сети HTM необходимо подавать информацию, непрерывно изменяющуюся во времени. Как и в случае восприятия информации человеческим мозгом, статичные данные не воспринимаются должным образом: например, органы слуха воспринимают изменяющийся во времени звуковой сигнал, а органы зрения распознают изображения посредством быстрых движений глаз (саккад).

Временные переходы между пространственными представлениями хранятся в регионах HTM с помощью связей между клетками. Такое свойство обеспечивает также реализацию непрерывного процесса предсказания одновременно с обучением. 

Когда регион HTM получает на вход информацию и конвертирует ее в пространственно-разреженное представление, в некотором наборе одна или несколько клеток переходят в состояние активности. Каждая клетка в регионе соединяется с ближайшими клетками с помощью описанных ранее дендритных сегментов и синапсов. При активации клетки формируются и усиливаются ее связи с клетками, которые были активными в предыдущий момент времени. 

Поэтому, когда сформированные соединения неактивной клетки становятся активными, она переходит в состояние предсказания. Если при следующих входных данных клетка, находящаяся в предсказательном состоянии, стала активной, то информация об ее активации сообщается далее соединенным с ней клеткам, тем самым приводя некоторые из них в состояние предсказания. 

Если при поступлении нового входа в пространственном представлении есть колонки, в которых ни одна из клеток не находилась в состоянии предсказания, то активизируютя все клетки в колонке.
То есть, если новые входные данные региона не совпадают с предсказанием, которое он сделал до этого, то регион понимает о поступлении неожиданной и не наблюдавшейся ранее информации. Это свойство активно используется для задачи обнаружения аномалий в data mining.

Процесс формирования связей и обучения происходит посредством изменения значений перманентности синапсов в дендритных сегментах клеток.

Таким образом, любой из регионов HTM на любом уровне иерархии запоминает последовательности пространственных и временных шаблонов, которые уже поступали к нему в качестве входных данных. Итоговым выходом всего региона является активность всех его клеток, включая клетки активные благодаря прямому воздействию входа и клетки активные в состоянии предсказания. Предсказания регионов низших слоев иерархии являются входными данными для регионов, находящихся выше по уровню, и за счет такой организации сигнал стабилизируется и обобщается при движении вверх по иерархии.
\section{Описание алгоритмов пакета nupic для языка программирования Python}
NuPIC (Numenta Platform for Intelligent Computing) --- проект с открытым исходным кодом, основанный на теории временной иерархической памяти. Основными возможностями этого програмного пакета является анализ изменяющихся во времени данных, а именно  запоминание временных шаблонов в данных, предсказывание будущих значений и обнаружение аномалий. 

Реализация моделей HTM основывается на описанных далее алгоритмах.

\subsection{Spatial Pooler} 
Информация, поступающая в сеть HTM кодируется с помощью встроенных в NuPIC кодировщиков в бинарное представление. Далее закодированная информация, поступающая от сенсорных источников или от низших по иерархии регионов, поступает в Spatial Pooler (далее SP). Основной задачей для SP является конвертация входных данных в пространственно разреженное представление. 

Фиксированное число колонок воспринимает активные входные биты с помощью потенциальных синапсов дендритного сегмента: синапс будет считаться активным если значение его перманентности превысит заданный порог. При этом, чтобы оставить свойство разреженности представлений данных, выбирается некоторое число колонок, а ближайшие к ним подавляются. Выбор определенной колонки в качестве подавляющей зависит от числа ее активных синапсов и тем, насколько колонка чаще бывает активной относительно других колонок в регионе. На этом этапе также происходит обучение: для активной колонки значение перманентности синапсов, подключенных к активным входным битам, увеличивается, а у остальных --- наоборот. Причем, достигая некоторого минимального порога, перманентность может полностью обнулиться.

В SP существует механизм регулирования распределения активности колонок. Если какая-то из колонок редко бывает активность относительно других, она увеличивает восприимчивость своих синапсов к входным данным, пока не станет одной из подавляющих колонок, таким способом вынуждая другие колонки представлять немного другие входные данные.

Алгоритм SP в NuPIC можно также использовать для классификации данных, не изменяющихся последовательно во времени.

\subsection{Temporal Pooler}
Задачами Temporal Pooler  (далее TP) является запоминание временных шаблонов и предсказание. Памятью обладает каждая клетка, но не система в целом. 
Запоминание происходит посредством связей между активными клетками и клетками, активными в предыдущий момент времени. Предсказание будущей активности также осуществляется самой клеткой, основываясь на частоте появления таких связей. 
TP начинает работать с пространственно-разреженными представлениями сразу после SP. Если в активной колонке есть клетки в состоянии предсказания, то они активизируются, иначе активизируются все клетки колонки. Множество активных клеток представляет закодированные входные данные с учетом прошлых данных. Далее для всех дендритных сегментов клеток в регионе считаем число синапсов с активными клетками. Дендритные сегменты, для которых полученное число превзошло  фиксированное пороговое значение, переводят клетку, которой они принадлежат, в состояние предсказания. Значения синапсов в активных дендритных сегментах увеличивается, а в остальных уменьшается.

% Для предсказаний больше чем на 1 шаг существует система временных изменений перманентности
%Идея, лежащая в основе реализации, заключается в следующем: смежные по времени пространственные шаблоны в данных возможно имеют общую причину, и поэтому будет формироваться устойчивое представление серии пространственных шаблонов. 

\subsection{Swarming} 
Одним из основных алгоритмов в NuPIC также является Swarming algorithm, который создает для данных несколько различных моделей и подбирает наилучшую, то есть с наименьшей ошибкой предсказания. Параметрами моделей могут быть различные кодировщики, параметры Spatial и Temporal Pooler, различные классификаторы, а также параметры входных данных, такие как рассматриваемые признаки. Поэтому множество возможных моделей, среди которого будем искать оптимальную, огромное, и Swarming запускает N процессов параллельно.